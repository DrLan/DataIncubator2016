{"name":"Data Incubator 2016","tagline":"Hong's Application for Data Incubator 2016 Fellow","body":"### *Project Proposal:* **Dr.Job -- A Smart Job Recommendation Engine**\r\n\r\nFresh graduates face many problems while applying for jobs and are having millions of doubts in their mind. It not hard to believe that fresh graduates, even Ph.D. graduate don’t know what career paths lie in front of them at the moment of job hunting. A Ph.D. who got training in engineering might be a good match for a business analyst position with the analytical skills he/she gained during more than 5 years in graduate school. But the real story is many of them didn’t even think of looking for jobs in fields outside of  their tiny research area. \r\n\r\nIronically, the difficulties of finding the perfect jobs lie not in the scarcity of job information, but overwhelming of online information for job hunting thanks to the blooming of information technology. Numerous websites exist for displaying job postings, among them are Monster.com, Indeed.com, and CareerBuilder.com, in addition to LinkedIn.com job section. One search of “Data Scientists” on Indeed.com gives you 20,000+ results, let alone other websites. It’s just impossible to click and check all the positions by hand. Although all of these websites provide search results filter feature to help ease the job hunting process, it didn’t solve the fundamental problem of identifying the right career,and therefore choose the correct key words. Some of these websites even provide preliminary job recommendation features such as that offered by CareerBuilder.com. However, a recommender system like that makes job or company recommendations based primarily on users’ browsing history, rather than on the true background of the users. \r\n\r\n\r\nThe idea proposed here is to design a smart web and app platform for job seeker to find the right jobs with the help of machine learning and historical employment data.  There are subtle difference between a “dream job” and a “best-match job”. The purposes of this proposed job recommendation engine include:\r\nto give user a clear image of where they stand among other applicants by comparing their profiles\r\nto provide users a summary of job statistics, such as where has the most job opening, and what skill sets do they require\r\nto introduce a “Job Match Rating System” that utilize latent semantics based information retrieval techniques to rate job postings so that users can focus on jobs that suit them best.\r\nto provide a profile refining recommendation based on the difference between user profile of requirements of their dream jobs. \r\nThe part of the summary of job statistics is implemented as a demo for this Data Incubator challenge problem. The rest will be completed at 2016 Data Incubator Cohort. \r\n\r\n\r\n### Describe Data Source\r\nHistorical employment data is scraped from job posting and professional social network websites by specially designed \"web spiders\". Also job postings are collected from various job posting websites. [Indeed.com](www.indeed.com) and [LinkedIn](www.linkedin.com) also provide APIs for job searching. These features may be employed in the future.\r\nPython Spider code for collecting data used in this project so far is attached:\r\n~~~\r\n# -*- coding: utf-8 -*-\r\n\r\n#-------------------------------------------------------------------------------------------------\r\nimport urllib2\r\nfrom bs4 import BeautifulSoup\r\nimport pandas as pd\r\nfrom nltk.corpus import stopwords\r\nimport re\r\nimport pickle\r\nfrom time import sleep # To prevent overwhelming the server between connections\r\nfrom collections import Counter # Keep track of our term counts\r\n\r\n\r\n# create a dataframe of job listings from lists of the titles, companies, locations, and links\r\ndef createJobListingsDF(titles,companies,cities,states,links):#,descriptions):\r\n    jobDict = {\r\n    'Title':titles,\r\n    'Company':companies,\r\n    'City':cities,\r\n    'State':states,\r\n    'Link':links\r\n    #'Description':descriptions\r\n    }\r\n    \r\n    jobListings = pd.DataFrame(jobDict)\r\n    return jobListings\r\n\r\n    \r\n# Webscraping Indeed.com    \r\n#-------------------------------------------------------------------------------------------------\r\n# generate a list with all page urls\r\ndef countIndeedJobs(nPostsToShow):    \r\n    # combine base url with user-defined search terms\r\n    # first page    \r\n    baseUrl = 'http://www.indeed.com/jobs?q=' + search_url +'&filter=0&start='\r\n    pagesUrl = urllib2.urlopen(baseUrl)\r\n    soup = BeautifulSoup(pagesUrl)\r\n    # get the total number of all pages \r\n    numberInString = soup.find('div', {'id':'searchCount'}).text[16:]\r\n    \r\n    totalListings = int(numberInString.replace(',',''))\r\n    #nPostsToShow = 30\r\n    print min(nPostsToShow,totalListings)\r\n    pages = range(0, min(nPostsToShow,totalListings), 10)\r\n    myUrls = []\r\n    # generate urls for page 2 and above\r\n    for apage in pages:\r\n        myUrls.append(baseUrl + str(apage))\r\n    return myUrls\r\n\r\n# Parse webpage and return lists containing titles, companies, cities, states, and links\r\ndef getIndeedPage(aUrl):\r\n    jobsPage = urllib2.urlopen(aUrl)\r\n    soup = BeautifulSoup(jobsPage)\r\n    jobs = soup.findAll('td',{'id':'resultsCol'})\r\n    titles = []\r\n    companies = []\r\n    cities = []\r\n    states = []\r\n    links = []\r\n    #descriptions =[]\r\n    for job in jobs:\r\n        titleBlocks = job.findAll('div',{'itemtype':'http://schema.org/JobPosting'})\r\n        for titleBlock in titleBlocks:\r\n            title = titleBlock.find('a')['title']\r\n            titles.append(title)\r\n            link = 'www.indeed.com' + titleBlock.find('a')['href']\r\n            links.append(link)\r\n        companyBlocks = job.findAll('span',{'itemtype':'http://schema.org/Organization'})            \r\n        for companyBlock in companyBlocks:            \r\n            company = companyBlock.get_text('span',{'itemprop':'name'})\r\n            companies.append(company)\r\n        locationBlocks = job.findAll('span',{'itemtype':'http://schema.org/Postaladdress'})            \r\n        for locationBlock in locationBlocks:            \r\n            location = locationBlock.get_text('span',{'itemprop':'addressLocality'})\r\n            city, space, state = location.partition(', ')            \r\n            cities.append(city)\r\n            states.append(state[:2])       \r\n    return [titles,companies,cities,states,links]\r\n\r\n# Iterate through all webpages and convert lists to dataframe\r\ndef getIndeedJobs(nPostsToShow,titles = [],companies = [],cities = [], states = [],links = []):\r\n    myUrls = countIndeedJobs(nPostsToShow)\r\n    statusCount=0 \r\n    for aUrl in myUrls:\r\n        data = getIndeedPage(aUrl)\r\n        #print data[0]\r\n        titles = titles + data[0]\r\n        companies = companies + data[1]\r\n        cities = cities + data[2]\r\n        states = states + data[3]\r\n        links = links + data[4]\r\n        statusCount += 1\r\n        print statusCount\r\n    print 'start grabbing indeed.com jobs...'\r\n    jobDict = {\r\n    'Title':titles,\r\n    'Company':companies,\r\n    'City':cities,\r\n    'State':states,\r\n    'Link':links\r\n    #'Description':descriptions\r\n    }\r\n    return jobDict\r\n    #allJobs = createJobListingsDF(titles,companies,cities,states,links)\r\n    print 'finished grabbing indeed.com Jobs!'\r\n    #return allJobs\r\n    \r\ndef getIndeedJobsFromUrls(urls):\r\n    titles = []\r\n    companies = []\r\n    cities = []\r\n    states = []\r\n    links = []\r\n    myUrls = urls\r\n    statusCount=0 \r\n    for aUrl in myUrls:\r\n        data = getIndeedPage(aUrl)\r\n        #print data[0]\r\n        titles = titles + data[0]\r\n        companies = companies + data[1]\r\n        cities = cities + data[2]\r\n        states = states + data[3]\r\n        links = links + data[4]\r\n        statusCount += 1\r\n        print statusCount\r\n    print 'start grabbing indeed.com jobs...'\r\n    jobDict = {\r\n    'Title':titles,\r\n    'Company':companies,\r\n    'City':cities,\r\n    'State':states,\r\n    'Link':links\r\n    #'Description':descriptions\r\n    }\r\n    print 'finished grabbing indeed.com Jobs!'\r\n    return jobDict\r\n    #allJobs = createJobListingsDF(titles,companies,cities,states,links)\r\n    \r\n    #return allJobs\r\n\r\ndef getDescriptions(links):\r\n    descriptions =[]\r\n    for link in links:\r\n        description = getOneDescription(link)\r\n        descriptions.append(description)\r\n        print \"processed one more job link!\"\r\n    return descriptions\r\n    \r\ndef getOneDescription(link):\r\n    jobUrl = 'http://'+link\r\n    try:\r\n        page = urllib2.urlopen(jobUrl).read() # Connect to the job posting\r\n    except: \r\n        return\r\n        #page = urllib2.urlopen(jobUrl)\r\n    _soupJob = BeautifulSoup(page)\r\n    for script in _soupJob([\"script\", \"style\"]):\r\n        script.extract() # Remove these two elements from the BS4 object\r\n    text=_soupJob.get_text()\r\n    #print text\r\n    #lines = (line.strip() for line in text.splitlines()) # break into lines\r\n    \r\n    lines = (line.strip() for line in text.splitlines()) # break into lines\r\n    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\r\n    def chunk_space(chunk):\r\n        chunk_out = chunk + ' ' # Need to fix spacing issue\r\n        return chunk_out  \r\n    text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\r\n    # Now clean out all of the unicode junk (this line works great!!!)\r\n    try:\r\n        text = text.decode('unicode_escape').encode('ascii', 'ignore') # Need this as some websites aren't formatted\r\n    except:                                                            # in a way that this works, can occasionally throw\r\n        return                                                         # an exception   \r\n    text = re.sub(\"[^a-zA-Z.+3]\",\" \", text)  # Now get rid of any terms that aren't words (include 3 for d3.js)\r\n                                                # Also include + for C++\r\n    text = text.lower().split()  # Go to lower case and split them apart\r\n    #print text\r\n    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\r\n    text = [w for w in text if not w in stop_words]\r\n    return text\r\n\r\n\r\n#if __name__ == \"__main__\":   \r\nkey_words = 'Data Scientist'          \r\nsearch_url = '+'.join(key_words.split())            # Set search terms for Indeed and CareerBuilder    \r\nnPostsToShow = 20000 # if inf : to show all job posts\r\nallUrls = countIndeedJobs(nPostsToShow)\r\njobArray=[]\r\nfor i in range(20):\r\n    #urlArray.append(url[i*100:100*(i+1)]\r\n    url1000 =allUrls[i*100:100*(i+1)]\r\n    job1000 = getIndeedJobsFromUrls(url1000)\r\n    jobArray.append(job1000)\r\n    print \"finished\"+str(1000*(i+1))\r\n    \r\ndescriptionArray=[]\r\ntitles=[]\r\ncompanies=[]\r\ncities=[]\r\nstates=[]\r\nlinks=[]\r\n\r\nfor i in range(19):\r\n    titles = titles + jobArray[i]['Title']\r\n    companies = companies + jobArray[i]['Company']\r\n    cities = cities + jobArray[i]['City']\r\n    states = states + jobArray[i]['State']\r\n    links = links + jobArray[i]['Link']\r\njobDict = {\"Title\":titles,\"Company\":companies,\"City\":cities,\"State\":states,\"Link\":links}\r\nwith open('jobDict.csv','wb') as f:\r\n    pickle.dump(jobDict,f)\r\n    \r\n    \r\nfilename = \"job\"+str(i+1)+\"000.csv\"\r\n\r\ndescriptionArray=[]\r\ndescriptions=[]\r\nallJobLinks = jobDict['Link']\r\nfor i in range(190):\r\n    url100 = allJobLinks[100*i:100*(i+1)]\r\n    description100 = getDescriptions(url100)\r\n    print \"processed:\"+str(i)+\" x100\"\r\n    descriptionArray.append(description100)\r\n    descriptions=descriptions+description100\r\nwith open('descriptions.csv','wb') as f:\r\n    pickle.dump(descriptions,f) \r\n    \r\nfor i in range(20):\r\n    #urlArray.append(url[i*100:100*(i+1)]\r\n    jobLink1000 = jobArray[i]['Link']\r\n    description1000 = getDescriptions(jobLink1000)\r\n    descriptionArray.append(description1000)\r\n    print \"finished\"+str(1000*(i+1))\r\n\r\n\r\njob_descriptions=descriptions    \r\ndoc_frequency = Counter() # This will create a full counter of our terms. \r\n[doc_frequency.update(item) for item in job_descriptions] # List comp\r\n    # Now we can just look at our final dict list inside doc_frequency\r\n    # Obtain our key terms and store them in a dict. These are the key data science skills we are looking for\r\nprog_lang_dict = Counter({'R':doc_frequency['r'], 'Python':doc_frequency['python'],\r\n                'Java':doc_frequency['java'], 'C++':doc_frequency['c++'],\r\n                'Ruby':doc_frequency['ruby'],\r\n                'Perl':doc_frequency['perl'], 'Matlab':doc_frequency['matlab'],\r\n                'JavaScript':doc_frequency['javascript'], 'Scala': doc_frequency['scala']})\r\n\r\nanalysis_tool_dict = Counter({'Excel':doc_frequency['excel'],  'Tableau':doc_frequency['tableau'],\r\n                    'D3.js':doc_frequency['d3.js'], 'SAS':doc_frequency['sas'],\r\n                    'SPSS':doc_frequency['spss'], 'D3':doc_frequency['d3']})  \r\n\r\nhadoop_dict = Counter({'Hadoop':doc_frequency['hadoop'], 'MapReduce':doc_frequency['mapreduce'],\r\n            'Spark':doc_frequency['spark'], 'Pig':doc_frequency['pig'],\r\n            'Hive':doc_frequency['hive'], 'Shark':doc_frequency['shark'],\r\n            'Oozie':doc_frequency['oozie'], 'ZooKeeper':doc_frequency['zookeeper'],\r\n            'Flume':doc_frequency['flume'], 'Mahout':doc_frequency['mahout']})\r\n\r\ndatabase_dict = Counter({'SQL':doc_frequency['sql'], 'NoSQL':doc_frequency['nosql'],\r\n                'HBase':doc_frequency['hbase'], 'Cassandra':doc_frequency['cassandra'],\r\n                'MongoDB':doc_frequency['mongodb']})\r\n            \r\noverall_total_skills = prog_lang_dict + analysis_tool_dict + hadoop_dict + database_dict # Combine our Counter objects\r\nfinal_frame = pd.DataFrame(overall_total_skills.items(), columns = ['Term', 'NumPostings']) # Convert these terms to a                                                                                                 # dataframe\r\n    # Change the values to reflect a percentage of the postings \r\nfinal_frame.NumPostings = (final_frame.NumPostings)*100/len(job_descriptions) # Gives percentage of job postings                                                                                    #  having that term \r\n    # Sort the data for plotting purposes\r\nfinal_frame.sort(columns = 'NumPostings', ascending = False, inplace = True)\r\n    # Get it ready for a bar plot\r\nfinal_plot = final_frame.plot(x = 'Term', kind = 'bar', legend = None, \r\n                            title = 'Percentage of Data Scientist Job Ads with a Key Skill ' )#+ city_title)\r\nfinal_plot.set_ylabel('Percentage Appearing in Job Postings')\r\nfig = final_plot.get_figure() # Have to convert the pandas plot object to a matplotlib object\r\n~~~\r\n\r\n### Figure for Collected Data\r\n![Key Skills for Data Scientist Position](https://drive.google.com/open?id=0B9FWcQkjA-1hU1pNMEk2YTlOLUNySTZtY1RDaUI2eHQ5aHZn)\r\n![Key skills for Data Scientist Position](https://drive.google.com/open?id=0B9FWcQkjA-1hU1pNMEk2YTlOLUNySTZtY1RDaUI2eHQ5aHZn)\r\n(https://drive.google.com/file/d/0B9FWcQkjA-1hU1pNMEk2YTlOLUNySTZtY1RDaUI2eHQ5aHZn/view?usp=sharing)\r\nhttps://drive.google.com/open?id=0B9FWcQkjA-1hU1pNMEk2YTlOLUNySTZtY1RDaUI2eHQ5aHZn\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}